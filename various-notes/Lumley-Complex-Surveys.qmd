---
title: "Complex Surveys: A Guide to Analysis Using R by Thomas Lumley"
author: "Justin Savoie"
format:
  html:
    embed-resources: true
    toc: true
    number-sections: true
editor: visual
---

These are my notes from the book. I don't distinguish between direct quotes from the book and my own comments. For illustrative purposes, I replicate some of the analysis using the package using basic algebra. This is a work in progress.

# Basic Tools

## Goals of inference

### Population or process

Most of statistics is model-bused (think regression,MRP). The analysis of complex survey samples, in contrast, is usually design-based. The model represents the process; can be generalized to other situations. The goal of the design-based analysis is to estimate features of the fixed population.

### Probability samples

The fundamental statistical concept in design-based inference is the probability sample or random sample. "Taking a random sample" of 1000 individuals means a sampling procedure when any subset of 1000people from the population is equally likely to be selected.

A stratified random sample is still a probability sample (e.g., taking a simple random sample of 20 people from each state).

What makes aprobability sample is the procedure for taking samples from a population.

-   Every individual in the population must have a non-zero probability of ending up in the sample (written $\pi_i$ for individual $i$)
-   The probability $\pi_i$ must be known for every individual
-   Every pair of individuals in the sample must have a non-zero probability of both ending up in the sample ($\pi_{ij}$)
-   The probability $\pi_{ij}$ must be known for every pair that does end up in the sample.

The first two properties are necessary in order to get valid population estimates; the last two are necessary to work out the accuracy of the estimates. If individuals were sampled independently of each other the first two properties would guarantee the last two (which is the case in the surveys I work with).

### Sampling weights

The fundamental statistical idea behind all of design-based inference is that an individual sampled with a sampling probability of $\pi_i$ represents $1/\pi_i$ individuals in the population. The value $1/\pi_i$ is called the sampling weight.

Given a sample of size n,the (Horvitz-Thompson) estimator $\hat{T}_X$ for the population total $T_X$ of $X$ is $$\hat{T}_X=\sum_{i=1}^{n}\frac{1}{\pi_i}X_i$$

The variance estimate is

$$\mathrm{var}[\hat{T}_X] = \sum_{i,j} \left( \frac{X_i X_j}{\pi_{ij}} - \frac{X_i}{\pi_i} \frac{X_j}{\pi_j} \right)$$ Knowing the formula for the variance estimator is less important to the applied user, but it is useful to note two things. The first is that the formula applies to any design, however complicated, where $\pi_i$ and $\pi_ij$ are known for the sampled observations. The second is that the formula depends on the pairwise sampling probabilities $\pi_ij$, not just on the sampling weights; this is how correlations in the sampling design enter the computations.

Statisticians use the term 'weight' to mean at least three different things.

1.  sampling weights: A sampling weight of 1000 means that the observation represents 1000 individuals in the population.

2.  precision weights: A precision (or inverse-variance) weight of 1000 means that the observation has 1000 times lower variance than an observation with a weight of 1.

3.  frequency weights: A frequency weight of 1000 means that the sample contains 1000 identical observations and space is being saved by using only one record in the data set to represent them.

In this book, weights are always sampling weights. They'll all produce the same point estimates but not same standard errors.

### Design effects

A complex survey will not have the same standard errors for estimates as a simple random sample of the same size, but many sample size calculations are only conveniently available for simple random samples.

The design effect was defined by Kish (1965) as the ratio of a variance of an estimate in a complex sample to the variance of the same estimate in a simple random sample.

Design effects are greater than 1.0 when the sample is not simple random, implying that a larger sample size is needed.

## An introduction to the data

The book uses some datasets. I think they are all available through the survey package.

## Obtaining the software

The book uses R.

## Using R

R is very nice, really!

# Simple and stratified sampling

## Analyzing simple random samples

The Horvitz-Thompson estimator of the population total of a variable X is

$$\hat{T}_X=\sum_{i=1}^{n}\frac{1}{\pi_i}X_i$$

A sampling weight $\pi_i$ is a value assigned to each unit in a sample, representing the number of units in the total population that the sampled unit represents. Let's say there are 31 million Canadian adults. Let's say, implausibly, that income is a normal centered at 50k with sd of 5k.

```{r}
weight <- 31000000/1000
population <- rnorm(31000000,50000,5000)
sample <- sample(population,1000)

(T_hat_X <- (31000000/1000)*sum(sample))
```

Which is a ok estimate of that true number

```{r}
(T_X <- sum(population))
```

The variance of this Horvitz-Thompson estimator is

$$\mathrm{var}[\bar{X}] = \frac{N - n}{N} \times N^2 \times \frac{\mathrm{var}[X]}{n}$$

```{r}
(var_T_hat_X <- ((31000000-1000)/31000000)*31000000^2*(var(sample)/1000))
```

For illustration purposes, you can get this estimator using simulations

```{r}
get_sum_one_sample <- function() (31000000/1000)*sum(sample(population,1000))
var(replicate(100000,get_sum_one_sample()))
```

The standard error of our estimator

```{r}
sqrt(var_T_hat_X)
```

The first term in the variance formula is the inite population correction. When the population is large, you can drop it.

The population mean of X can be estimated by dividing the estimated total by the population size, N

```{r}
(mu_x <- T_hat_X / 31000000)
```

The variance estimate is obtained by dividing the variance estimate for the total by $N^2$ (get the square root for standard error)

```{r}
sqrt(var_T_hat_X / 31000000^2)
```

Which is equal to getting the standard error of the mean with the usual formula

```{r}
sd(sample)/sqrt(1000)
```

The Horvitz-Thompson estimator of the population size is the sum of the sampling weights, in large surveys where the population size is known, the sampling weights are adjusted so that the estimated population size equals the true sample size.

### Confidence intervals

Confidence intervals for estimates are computed by using a Normal distribution for the estimate, ie, for a 95% confidence interval adding and subtracting 1.96 standard errors.

### Describing the sample to R

The Academic Performance Index is computed for all California schools based on standardised testing of students. The data sets contain information for all schools with at least 100 students and for various probability samples of the data. `?apisrs` for more info.

```{r}
library(survey)
data(api)
dim(apisrs)
```

The variable fpc in this data set contains the number 6194, the number of schools in California. `id=~1` says that individual schools were sampled. `fpc=~fpc` says that the variable called fpc in the data set contains the population size.

```{r}
srs_design <- svydesign(id=~1,fpc=~fpc,data=apisrs)
```

We can replicate the estimates and standard errors by hand:

```{r}
svytotal(~enroll, srs_design)
(6194/200)*sum(apisrs$enroll)
(svytotal_SE <- sqrt( ((6194-200)/6194)*6194^2*(var(apisrs$enroll)/200)) )
svymean(~enroll, srs_design)
mean(apisrs$enroll)
sqrt(svytotal_SE^2 / (6194^2))
```

If the population size is not specified it is necessary to specify the sampling probabilities or sampling weights. If we have the sampling weight but not the population, the standard error are slightly larger.

```{r}
svytotal(~stype, srs_design)
(svytotal_SE_E <- sqrt( ((6194-200)/6194)*6194^2*(var(as.numeric(apisrs$stype=="E"))/200)) )
(svytotal_SE_H <- sqrt( ((6194-200)/6194)*6194^2*(var(as.numeric(apisrs$stype=="H"))/200)) )
(svytotal_SE_M <- sqrt( ((6194-200)/6194)*6194^2*(var(as.numeric(apisrs$stype=="M"))/200)) )
svymean(~stype, srs_design)
sqrt(svytotal_SE_E^2 / (6194^2))
sqrt(svytotal_SE_H^2 / (6194^2))
sqrt(svytotal_SE_M^2 / (6194^2))
```

## Stratified sampling

Stratified sampling involves dividing the population up into groups called strata and drawing a separate probability sample from each one. Since a stratified sample is just a set of simple random samples from each stratum, the Horvitz-Thompson estimator of the total is just the sum of the estimated totals in each stratum and its variance is the sum of the estimated variances in each stratum.

```{r}
strat_design <- svydesign(id=~1,strata=~stype,fpc=~fpc,data=apistrat)
svytotal(~enroll,strat_design)
```

Off because use of stratas (treating like random sample)

```{r}
(6194/200)*sum(apistrat$enroll)
```

Of course, this is ugly and impossible to read but the point is that it works: total is just the sum of the estimated totals in each stratum and its variance is the sum of the estimated variances in each stratum

```{r}
(apistrat$fpc[apistrat$stype=="E"][1]/sum(apistrat$stype=="E")) *
  sum(apistrat$enroll[apistrat$stype=="E"]) +
(apistrat$fpc[apistrat$stype=="H"][1]/sum(apistrat$stype=="H")) *
  sum(apistrat$enroll[apistrat$stype=="H"]) +
(apistrat$fpc[apistrat$stype=="M"][1]/sum(apistrat$stype=="M")) *
  sum(apistrat$enroll[apistrat$stype=="M"])

sqrt( ((apistrat$fpc[apistrat$stype=="E"][1]-sum(apistrat$stype=="E"))/apistrat$fpc[apistrat$stype=="E"][1])*apistrat$fpc[apistrat$stype=="E"][1]^2*(var(apistrat$enroll[apistrat$stype=="E"])/sum(apistrat$stype=="E")) +
   ((apistrat$fpc[apistrat$stype=="H"][1]-sum(apistrat$stype=="H"))/apistrat$fpc[apistrat$stype=="H"][1])*apistrat$fpc[apistrat$stype=="H"][1]^2*(var(apistrat$enroll[apistrat$stype=="H"])/sum(apistrat$stype=="H")) +
     ((apistrat$fpc[apistrat$stype=="M"][1]-sum(apistrat$stype=="M"))/apistrat$fpc[apistrat$stype=="M"][1])*apistrat$fpc[apistrat$stype=="M"][1]^2*(var(apistrat$enroll[apistrat$stype=="M"])/sum(apistrat$stype=="M")))


```

## Replicate Weights

The standard error of a mean or any other population summary is, by definition, the standard deviation of that estimated summary across many independent samples of data.

```{r}
population <- rnorm(1000000)
draw_1000 <- function(){sample(population,1000)}
sample1 <- draw_1000()
sd(sample1)/sqrt(1000)
sd(replicate(1000,mean(draw_1000())))
```

The replicate-weight approach to estimating standard errors computes the standard deviation of the estimated summary across many partially independent subsets of the one sample, and extrapolates from this to the standard deviation between completely independent samples. Replicate weight methods require more computation than using the Horvitz- Thompson estimator but are easier to generalize to statistics other than the mean and total (at least for software designers). Are given as example: Balanced repeated replication, Fay's method, jackkknife and bootstrap.

An example with the bootstrap: take a sample of observations (or clusters) with replacement from each stratum and the sampling weight is multiplied by the number of times the observation appears in this sample.

```{r}
strat_design <- svydesign(id=~1,strata=~stype,fpc=~fpc,data=apistrat)
svymean(~enroll,strat_design)

enroll_E <- apistrat$enroll[apistrat$stype=="E"]
fpc_E <- apistrat$fpc[apistrat$stype=="E"][1]
enroll_H <- apistrat$enroll[apistrat$stype=="H"]
fpc_H <- apistrat$fpc[apistrat$stype=="H"][1]
enroll_M <- apistrat$enroll[apistrat$stype=="M"]
fpc_M <- apistrat$fpc[apistrat$stype=="M"][1]

draw1 <- function() {(fpc_E*mean(sample(enroll_E,length(enroll_E),replace=TRUE)) +
  fpc_H*mean(sample(enroll_H,length(enroll_H),replace=TRUE)) +
  fpc_M*mean(sample(enroll_M,length(enroll_M),replace=TRUE))) /
  sum(fpc_E+fpc_H+fpc_M)}

draws1000 <- replicate(1000,draw1())
mean(draws1000);sd(draws1000)

```

Very close. I love the bootstrap, it's so intuitive.

### Specifying replicate weights to R

Replicate-weight approach: computes the standard deviation of the estimated summary across many partially independent subsets of the one sample, and extrapolate from this to the standard deviation between completely independent samples. Let's skip for now.

### Creating replicate weights to R

```{r}
boot_design <- as.svrepdesign(strat_design,type='bootstrap',replicates=1000)
svymean(~enroll,boot_design)
```

But we've already done something similar by hand, above.

Again, the most obvious advantage of designs with replicate weights in the survey package is that it is possible to compute standard errors for differences between subpopulations for arbitrary statistics; to perform an analysis that has not been implemented in the survey package.

Using replicate weights (or the bootstrap as I show) it is only necessary to write code to compute the weighted point estimates; this code is re-run on all the sets of replicate weights to estimate standard errors, which is usually easier than writing code for the linearization standard error estimates.

There are other notes, for example, the bootstrap is straightforward only when all the strata are large.

## Other population summaries

Estimators, such as the median or a regression coefficient, are obtained by solving equations that can be written as population totals or means. Standard errors for these estimates can be obtained with replicate weights. Alternatively we can use linearization, dicussed in Appendix.

### Quantiles

The median is defined implicitly rather than explicitly: the estimated population size above and below the median are equal.

Bootstrapping them:

```{r}
chis_adult <- foreign::read.dta("~/Downloads/adult.dta") # get on book website
chis <- svrepdesign(variables=chis_adult[,1:418] ,
repweights=chis_adult [,420:499] , weights=chis_adult[,419], combined.weights=TRUE, type="other", scale=1, rscales=1)
svyquantile(~bmi_p,design=chis,quantiles=c(0.25,0.5,0.75))
```

```{r}
sum_w <- sum(chis_adult$rakedw0)
bmi_order <- order(chis_adult$bmi_p)
median_loc <- which(cumsum(chis_adult$rakedw0[bmi_order])>(sum_w)/2)[1]
chis_adult$bmi_p[bmi_order][median_loc]
median_loc <- which(cumsum(chis_adult$rakedw0[bmi_order])>(sum_w)/4)[1]
chis_adult$bmi_p[bmi_order][median_loc]
```

```{r}
boot <- function(){
  temp <- chis_adult[,c("rakedw0","bmi_p")]
  temp_boot <- temp[sample(1:nrow(temp),replace=TRUE),]
  sum_w <- sum(temp_boot$rakedw0)
  bmi_order <- order(temp_boot$bmi_p)
  median_loc <- which(cumsum(temp_boot$rakedw0[bmi_order])>(sum_w)/2)[1]
  temp_boot$bmi_p[bmi_order][median_loc]
}
v_means <- replicate(500,boot())
mean(v_means);sd(v_means)
```

SE is bigger than the result from svyquantile() but actually in line with what's in the book. Strange.

**I'm skipping many chapters. This is what I'm using most often.**

# Post-stratification raking calibration

This is chapter 7.

Raking by hand:

```{r}
library(tidyverse)
gender <- sample(c("M","F"),100,replace=TRUE)
age <- sample(c("Y","O"),100,replace=TRUE)
educ <- sample(c("HS","C","U"),100,replace=TRUE)

df <- data.frame(gender,age,educ)

design <- svydesign(ids = ~1, data = df, weights = ~rep(1, nrow(df)))

gender_marg <- data.frame(gender = c("M", "F"), Freq = c(40, 60))
age_marg <- data.frame(age = c("Y", "O"), Freq = c(30, 70))
educ_marg <- data.frame(educ = c("HS", "C", "U"), Freq = c(40, 40, 20))

raked_design <- rake(design, 
                     sample.margins = list(~gender, ~age, ~educ), 
                     population.margins = list(gender_marg, age_marg, educ_marg))

df$weights <- weights(raked_design)
stored_w <- df$weights
df$weights <- 1
store_weights <- df$weights
for (i in 1:100){
  mult_gender <- df %>% 
    group_by(gender) %>%
    summarise(sw=sum(weights)) %>%
    left_join(gender_marg,"gender") %>% mutate(mult=Freq/sw)
  mult_age <- df %>% 
    group_by(age) %>%
    summarise(sw=sum(weights)) %>%
    left_join(age_marg,"age") %>% mutate(mult=Freq/sw)
  mult_educ <- df %>% 
    group_by(educ) %>%
    summarise(sw=sum(weights)) %>%
    left_join(educ_marg,"educ") %>% mutate(mult=Freq/sw)
  
  df <- df %>%
    left_join(mult_gender,"gender") %>%
    left_join(mult_age,"age") %>%
    left_join(mult_educ,"educ") %>%
    mutate(weights=weights*mult.x*mult.y*mult) %>%
    select(gender,age,educ,weights) %>%
    mutate(weights=100*(weights/sum(weights)))
  if (sum((df$weights-store_weights)^2)<0.00001){
    break
  }
  store_weights <- df$weights
  
}

cor(stored_w,df$weights)

```

Standard errors: 