---
title: "[Multiple linear regression](index.html)"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
editor: visual
---

## OLS with matrices

The [linear regression model](https://en.wikipedia.org/wiki/Linear_regression) takes the form: $$y=\beta_0+\beta_1x_1+\beta_2x_2...+\beta_kx_{k}+\varepsilon$$

where $x_1,x_2...x_k$ are the predictor variables and $\beta_1,\beta_2...\beta_k$ are the coefficients.

In matrix form: $$y=X\beta+\varepsilon$$

where $y$ is a $n \times 1$ vector of dependent variable, $X$ is $n \times (p+1)$ with the first columns as 1s that multiply the intercept $\beta_0$, $\beta$ is a \$(p+1)\times 1 \$ vector of regression coefficients, including the intercept, \varepsilon is a $n \times 1$ vector of residuals (or errors).

This model returns a prediction which is essentially a weighted average of the independent variables, plus an intercept which represents the expected value of the dependent variable when all predictors are set to zero.

In `R`, let's model `api00` as a function of `enroll`, `meals` and `full`:

```{r}
d <- read.csv("https://stats.idre.ucla.edu/wp-content/uploads/2019/02/elemapi2v2.csv")
fit <- lm(api00 ~  enroll + meals + full, data = d)
X <- as.matrix(cbind(1,d[,c("enroll","meals","full")]))
y <- d$api00
```

The vector of coefficients $\hat{\beta}$ can be obtained using matrix operations: $$(X^TX)^{-1}X^Ty$$

```{r}
betas <- solve(t(X) %*% X) %*% t(X) %*% y
```

The [variance](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) of $\hat{\beta}$ is $Var(\hat{\beta})=\hat{\sigma^2}(X^TX)^{-1}$

```{r}
# Get the sigma hat squared (residual_variance)
residuals <- y - (X %*% betas)
k <- ncol(X)-1
degrees_of_freedom <- nrow(X) - k - 1
residual_variance <- sum(residuals^2) / degrees_of_freedom 
# Multiply it as in the equation
betas_cov <- residual_variance * solve(t(X) %*% X)
betas_se <- sqrt(diag(betas_cov))
cbind(c(betas),unname(c(betas_se)))
coef(summary(fit))[,1:2]
```

## Maximum likelihood estimation

With $y=X\beta+\varepsilon$:

$$L(\beta, \sigma^2 | Y, X) = \prod_{i=1}^{n} {\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {y_i-X_i\beta }{\sigma }}\right)^{2}}$$

With a [little bit of math](https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood) we can get the log likelihood:

$$lnL(\beta, \sigma^2 | Y, X) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-X_i\beta)$$

This is something we can optimize in R:

```{r}
betas_params_initial <- rnorm(4)
sigma_params_initial <- rlnorm(1)
params <- c(betas_params_initial,sigma_params_initial)
to_optim <- function(pars,y,x1,x2,x3){
  b0 <- pars[1]
  b1 <- pars[2]
  b2 <- pars[3]
  b3 <- pars[4]
  sigma_u <- pars[5]
  sigma <- exp(sigma_u)
  mu_vector <- b0+b1*x1+b2*x2+b3*x3
  n<-length(y)
  -sum(sapply(1:n, function(x) {dnorm(y[x],mu_vector[x],sigma,log=TRUE)}))
  # alternatively without using dnorm
  #log_density <- function(x, mean, sd) {-0.5 * log(2 * pi) - log(sd) - 0.5 * ((x - mean)/sd)^2}
  #-sum(sapply(1:n, function(x) {log_density(y[x], mu_vector[x], sigma)}))
}
optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="L-BFGS-B")$par
coef(summary(fit))[,1]
```

MLE of multiple regression minimizes the sum of squares residuals, so we could optimize that:

```{r}
betas_params_initial <- rnorm(4)
paramsDirectSum <- betas_params_initial
to_optimDirectSum <- function(pars,y,x1,x2,x3){
  b0 <- pars[1]
  b1 <- pars[2]
  b2 <- pars[3]
  b3 <- pars[4]
  mu_vector <- b0+b1*x1+b2*x2+b3*x3
  n<-length(y)
  sum((y-mu_vector)^2)
}
optim(paramsDirectSum,to_optimDirectSum,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="L-BFGS-B")$par
```

Optimization does not always work:

```{r}
(nelder_mead_params <- optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="Nelder-Mead")$par)
(BFGS_params <- optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="BFGS")$par)
coef(summary(fit))[,1]
```

I'm not sure why Nelder-Mead is so bad. Maybe it found a local minimum.

To obtain the standard errors, we need the Hessian matrix of the second derivatives of the log-likelihood function evaluated at the maximum likelihood estimates. Here, I need to brush up. I will write something in the future about gradients, jacobians, hessians and Newton's method for MLE.

The inverse of this matrix is an estimate of the covariance matrix of the parameter estimates.

```{r}
fit_optim <- optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="L-BFGS-B",hessian = TRUE)
cov_beta_hat <- solve(fit_optim$hessian)
(se_beta_hat <- sqrt(diag(cov_beta_hat)))
coef(summary(fit))[,2]
```

We can also do the optimization by hand using [Netwon-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization). It involves updating parameters by subtracting the multiplication of the inverse of the Hessian with the gradient vector.

```{r}
betas <- params[1:4]

for (i in 1:5){
  yhat = X %*% betas
  gradients <- t(X) %*% (y-yhat)
  hessian <- - t(X) %*% X
  betas <- betas - solve(hessian) %*% gradients
}

residuals <- y - X %*% betas
sigma2_hat <- sum(residuals^2) / (length(y) - length(betas))
cov_beta_hat <- sigma2_hat * solve(-hessian)
se_beta_hat <- sqrt(diag(cov_beta_hat))
cbind(c(betas),unname(c(betas_se)))
coef(summary(fit))[,1:2]
```

Finally, this is used in Machine Learning more than in social sciences but it's helpful to know ; here's how to get the point estimates of the coefficients using gradient descent.

For gradient descent, we need to scale fist.

```{r}
X <- as.matrix(cbind(1, d[, c("enroll", "meals", "full")]))
X[,-1] <- scale(X[,-1])
y <- scale(d$api00)
m <- length(y)

# Settings
alpha <- 0.001  # Learning rate
num_iterations <- 10000
betas <- matrix(rnorm(4,0,0.1), ncol(X), 1)  # Initialize beta values

# Gradient Descent in a for loop
for(i in 1:num_iterations){
  # Compute the prediction
  prediction <- X %*% betas
  
  # Compute the error
  error <- prediction - matrix(y, ncol=1)
  
  # Update betas
  for(j in 1:ncol(X)){
    gradient <- t(X[, j]) %*% error
    betas[j,] <- betas[j,] - alpha * (1/m) * gradient
  }
}

round(print(betas),3)
round(coef(lm(y~X[,-1])),3)

```

It worked but I had to play with the alpha number of iterations parameters.

## Additional statistics lm()

```{r}
summary(fit)

(R2 <- 1 - (sum((fit$model$api00 - predict(fit))^2) /
  sum((fit$model$api00-mean(fit$model$api00))^2)))
(R2adj <- 1 - (1 - R2) * ((nrow(fit$model) - 1) / 
                           (nrow(fit$model) - length(fit$coefficients[-1]) - 1)))
(Residuals <- quantile(fit$residuals,c(0,0.25,0.5,0.75,1)))
(tstats <- coef(summary(fit))[,1] / 
  coef(summary(fit))[,2])
(d_free <- nrow(fit$model) - length(fit$coefficients))
(pvalues <- 2 * (1 - pt(abs(tstats), df=d_free)))

(Res_se <- sqrt(sum(fit$residuals^2) /
       (nrow(fit$model)-(1+length(fit$coefficients[-1])))))

SSE=sum(fit$residuals^2)
SSyy=sum((fit$model$api00-mean(fit$model$api00))^2)
k<-length(fit$coefficients)-1
Fstat <- ((SSyy-SSE)/k) / (SSE/(400-(3+1)))

(Fstat_pval <- 1-pf(Fstat,3,396))
```

## Additional statistics glm() for the Gaussian family

```{r}
fit_glm <- glm(api00 ~  enroll + meals + full, data = d, family='gaussian')
summary(fit_glm)

quantile(fit_glm$residuals,c(0,0.25,0.5,0.75,1))

(Dispersion <- sum(fit$residuals^2) /
  (nrow(fit$model)-(1+length(fit$coefficients[-1]))))
```

It's possible to get the loglik from brute force:

```{r}
(loglikMLE <- sum(
  sapply(1:400, function(i){
    log(dnorm(fit_glm$model$api00[i],
              coef(fit_glm)[[1]] + sum(fit_glm$model[i,2:4] * coef(fit_glm)[2:4]),
              sqrt(Dispersion)))
  })
))
logLik(fit_glm)
```

It's possible to get the loglik using a formula derived from summation:

```{r}
(nulldeviance <- sum((d$api00 - mean(d$api00))^2))
(residualdeviance <- sum(residuals(fit_glm)^2))

(loglikDirect <-
    (-400/2) * log(2*pi) - (400/2) * log(Dispersion) -
    (1/(2*Dispersion)) * residualdeviance)
```

4 coefficient parameters + dispersion of Gaussian = 5

```{r}
(AIC = 2*5 - 2*logLik(fit_glm))
BIC(fit_glm)
unname(5*log(400) - 2*logLik(fit_glm))

```

I will discuss Fisher Scoring in another entry.

## Confidence and prediction intervals

```{r}
(as.matrix(cbind(1,fit$model[,-1])) %*% coef(fit))[1:10]
predict(fit)[1:10]
```

When using `predict(fit, se.fit = TRUE)`, we can derive either a confidence interval or a prediction interval based on the standard error of the prediction. They are [different](https://robjhyndman.com/hyndsight/intervals/). The confidence interval reflects the uncertainty surrounding the estimated average value at the population level, and it would narrow indefinitely with infinite data. The prediction interval has inherent randomness and represents predictions for individual data points.

First we need to calculate the [standard error of the prediction](https://stackoverflow.com/questions/38109501/how-does-predict-lm-compute-confidence-interval-and-prediction-interval/38110406#38110406).

```{r}
fit <- lm(api00 ~ enroll + meals + full, data = d)
Xp <- as.matrix(cbind(1,fit$model[,-1]))
b <- coef(fit)
yh <- Xp %*% b
V <- vcov(fit)
var.fit <- rowSums((Xp %*% V) * Xp)
sqrt(var.fit)[1:10]
predict(fit,se.fit=TRUE)$se.fit[1:10]
```

Once we have the standard error of the prediction, we can get the [confidence and prediction](https://stats.stackexchange.com/questions/154247/what-are-the-formulae-used-in-r-by-predict-lm-when-interval-a-none-b-pred) intervals as follow:

Confidence interval is self-explanatory:

```{r}
predit_fit <- predict(fit,se.fit=TRUE)
rbind(predit_fit$fit[1:10] - predit_fit$se.fit[1:10]*qt(0.975,396),
      predit_fit$fit[1:10] + predit_fit$se.fit[1:10]*qt(0.975,396))
head(predict(fit,interval="confidence"),10)
```

This is similar but adding randomness based on the residual error:

```{r}
rbind(predit_fit$fit[1:10] - qt(0.975,396)*sqrt(predit_fit$se.fit[1:10]^2+sum(fit$residuals ^ 2) / fit$df.residual),
      predit_fit$fit[1:10] + qt(0.975,396)*sqrt(predit_fit$se.fit[1:10]^2+sum(fit$residuals ^ 2) / fit$df.residual))
head(predict(fit,interval="prediction"),10)
```
