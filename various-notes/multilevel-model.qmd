---
title: "[What is a linear mixed-effects model? The basic intuiton](index.html)"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
editor: visual
---

For intuition, the way I think about linear mixed-effects model is by starting from the equivalent non-mixed normal linear regression model.

The [Bates lme4 book](https://stat.ethz.ch/\~maechler/MEMo-pages/lMMwR.pdf) uses the Dyestuff dataset to model yield (the production of a harvest) as a function of Batch (which fertiliser). It's a really simple design: 6 groups with samples and an outcome.

The simple linear regression model with one dummy (minus one because there's an intercept) per group gives the average yield per group. The intercept gives the average of the first group; the intercept plus the other coefficients gives the average of the other groups.

```{r,warning=FALSE,message=FALSE}
library(lme4)
fit_fe <- lm(Yield~Batch,Dyestuff)
c(fit_fe$coefficients[1],
  fit_fe$coefficients[1]+
    fit_fe$coefficients[2:6])
tapply(Dyestuff$Yield, Dyestuff$Batch, mean)
```

Random effects does partial pooling. In this simple model, we still estimate a mean per group, but we now have one general intercept (for all groups not for the reference group) and one random intercept for each group representing how much it deviates from the fixed intercept. The partial pooling part means that information is shared between the groups. All estimates (here of the mean of each group) will be closer to the overall mean than with fixed effects. Multilevel models are conservative and reduce variance by increasing bias. Here it will not differ much because it's a simple balanced design.

```{r}
summary(fit_re <- lmer(Yield~(1|Batch),Dyestuff))
```

We can obtain the group averages with this mixed-effects model either by using the predict function or by manually adding the random intercepts to the fixed intercept.

```{r}
predict(fit_re,newdata=data.frame(Batch=LETTERS[1:6]))
c(fixef(fit_re)+ranef(fit_re)$Batch)[[1]]
```

In all cases (even batch B with a difference of 0.11 even if not visible) the estimate from the random effects model is more conservative.

```{r,warning=FALSE,message=FALSE}
library(tidyverse)
tibble(re=predict(fit_re,newdata=data.frame(Batch=LETTERS[1:6])),
       fe=predict(fit_fe,newdata=data.frame(Batch=LETTERS[1:6])),
       batch=LETTERS[1:6]) |>
  gather(key,value,-batch) |>
  ggplot(aes(x=value,y=batch,color=key)) + 
  geom_point() + labs(x="yield",color="model")
```

Here the groups are balanced so the difference is small. There's another example at the end of [this note](Being-Bayesian-Frequentist.html) where some groups have a very small n and it makes a little bit more difference. 

In fixed effects we calculate directly the mean of each group (intercept + how the other groups deviate with a fixed eff coefficient). In random effects, we get the grand mean (intercept), then look at how much each group deviates from that mean (ranef). Then we treat each ranef as a data point drawn from a normal mu=0 and we calculate only the sd/var of that normal

Random intercepts is a much much more parsimonious model, but sometimes it's hard to estimate/fit (not here but in more complicated models). Also, random intercepts is a biased estimator. It's easily shown in the above example, the predicted averages are actually off from what would be our best guess if we just averaged. In the end, FE and RE can be similar for some data (like here). If we had hundreds of groups random effects might make more sense. 

Here's how to calculate other the "random effects" variance in the summary output. More details [here](https://stats.stackexchange.com/questions/68106/understanding-the-variance-of-random-effects-in-lmer-models).

```{r}
(1/6 * sum((arm::se.ranef(fit_re)$Batch)^2)) +
  var(ranef(fit_re)$Batch)
# The Variance of the Residual is the within-subject variance
mean(tapply(Dyestuff$Yield, Dyestuff$Batch, var))
```

In the book (p.10) he gives an example of a degenerate random effect model. The fit is singular:

```{r}
lmer(Yield~(1|Batch),Dyestuff2)
```

That model doesn't explain anything (we can get the same information by looking at the f-test from the equivalent fixed-effects model). We should run `lm(Yield~1,Dyestuff2)` instead.

Mixed-effects models are "mixed" because they can never only be "random". The moment you have one random effect, you also need a fixed effect.

Beyond that we can combined fix effects and random effects in various ways. Using mtcars, random intercept for cyl and fixed slope for hp and fixed effect intercept (the overall intercept).

```{r}
lmer(mpg~hp+(1|cyl),mtcars)
```

Here, we have a fixed effect intercept, a fixed effect slope for hp. Both an intercept and a slope for hp are allowed to vary randomly across levels of cyl. In other terms each level of cyl as an intercept and a slope.

```{r}
re_mtcars<-lmer(mpg ~ hp + (1 + hp|cyl), mtcars)
```

I wrote at the beginning that I use linear models for intuition. This model is "parallel" to the model with an interaction between hp and cyl.

```{r}
fe_mtcars<-lm(mpg ~ hp*factor(cyl), mtcars)
```

Both give use one intercept per group:

```{r}
fixef(re_mtcars)[1]+ranef(re_mtcars)$cyl[,1]
c(coef(fe_mtcars)[1],coef(fe_mtcars)[1]+coef(fe_mtcars)[3:4])
```

Both give use one slope for hp by group:

```{r}
fixef(re_mtcars)[2]+ranef(re_mtcars)$cyl[,2]
c(coef(fe_mtcars)[2],coef(fe_mtcars)[2]+coef(fe_mtcars)[5:6])
```