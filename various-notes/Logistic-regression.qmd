---
title: "[Logistic regression](index.html)"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
editor: visual
---

## Definition

Rather than being drawn from a normal distribution (where $\mu=\beta X$) like in linear regression, the outcome variable in [logistic regression is drawn from a Bernouilli]((https://en.wikipedia.org/wiki/Logistic_regression#Definition)).

$$p(x)={\frac {1}{1+e^{-(\beta _{0}+\beta _{1}x)}}}$$ The logit function, or the log of the odds is defined as $$logit\:p(x)=\ln \left({\frac {p(x)}{1-p(x)}}\right)=\beta _{0}+\beta _{1}x$$

The likelihood (the probability of observing the data) is $$L=\prod _{k:y_{k}=1}p_{k}\,\prod _{k:y_{k}=0}(1-p_{k})$$

The log likelihood is

$$\ell =\sum _{k:y_{k}=1}\ln(p_{k})+\sum _{k:y_{k}=0}\ln(1-p_{k})$$

## Estimation

For example, in R,

$$p(vs=1) \sim Bernouilli(logit(\beta0+\beta1*mpg+\beta2*hp))$$

```{r}
fit<-glm(vs~mpg+hp,mtcars,family='binomial')
```

Optimizing:

```{r}
pars_init <- rnorm(3)
y <- mtcars$vs
X <- as.matrix(cbind(1, mtcars[, c("mpg", "hp")]))
to_optim <- function(pars, y, X) {
  betas <- pars
  t <- X %*% betas
  p <- plogis(t)
  ll1 <- sum(log(p[y == 1] + 1e-10))
  ll0 <- sum(log(1 - p[y == 0] + 1e-10))
  return(-(ll1 + ll0))
}
coef(fit)
optim(pars_init, to_optim, y = y, X = X,method = "BFGS")$par
```

It doesn't work. Looks like it's an optimization problem.

Scaling fixes it.

```{r,message=FALSE}
library(tidyverse)
X <- as.matrix(cbind(1, mtcars[, c("mpg", "hp")]%>%mutate_all(scale)))
fit_s<-glm(vs~scale(mpg)+scale(hp),mtcars,family='binomial')
fit_optim <- optim(pars_init, to_optim, y = y, X = X,method = "BFGS",hessian = TRUE)
cov_beta_hat <- solve(fit_optim$hessian)
se_beta_hat <- sqrt(diag(cov_beta_hat))
coef(summary(fit_s))[,1:2]
cbind(fit_optim$par,se_beta_hat)

```

Doing the optimization by hand with Newton Raphson

```{r}
newton_raphson_se <- function(pars, y, X, max_iter=100, tol=1e-06) {
  betas <- pars
  for(i in 1:max_iter) {
    p <- plogis(X %*% betas)
    W <- diag(as.vector(p * (1 - p)))
    gradient <- t(X) %*% (y - p)
    hessian <- -t(X) %*% W %*% X
    betas_new <- betas - solve(hessian, gradient)
    if(sum(abs(betas_new - betas)) < tol) {
      hessian_at_mle <- -t(X) %*% diag(as.vector(p * (1 - p))) %*% X
      cov_beta <- solve(-hessian_at_mle)
      se_beta <- sqrt(diag(cov_beta))
      return(list(coefficients=betas_new, std_error=se_beta, covariance=cov_beta))
    }
    betas <- betas_new
  }
  stop("Algorithm did not converge")
}
pars_init <- rnorm(3,0,0.1)
nr <- newton_raphson_se(pars_init, y, X)
coef(summary(fit_s))[,1:2]
cbind(nr$coefficients,nr$std_error)

```

Getting residuals:

```{r}
fit<-glm(vs~mpg+hp,mtcars,family='binomial')
summary(fit)
y <- mtcars$vs
pred_p <- predict(fit,type = 'response')
# deviance residuals
sign(y-pred_p) * ifelse(y==1,sqrt(-2*log(pred_p)),sqrt(-2*log(1-pred_p)))
resid(fit)
# working residuals
fit$residuals
(y-pred_p) / (pred_p*(1-pred_p))
quantile(resid(fit),c(0,0.25,0.5,0.75,1))
```

Getting deviance:

```{r}
# Residual deviance
sum(resid(fit))
# Null deviance is residual deviance from Null model
sum(resid(glm(vs~1,mtcars,family='binomial'))^2)
```

Getting log lik:

```{r}
logLik(fit)
log(prod(ifelse(y,pred_p,1-pred_p)))
```

Getting AIC:

```{r}
(AIC = 2*3 - 2*logLik(fit))
```



```{r}
Xp <- as.matrix(cbind(1,fit$model[,-1]))
b <- coef(fit)
yh <- Xp %*% b
V <- vcov(fit)
var.fit <- rowSums((Xp %*% V) * Xp)
sqrt(var.fit)[1:10]
predict(fit,se.fit=TRUE)$se.fit[1:10]
```

Confidence intervals:

```{r}
predict_fit <- predict(fit,se.fit=TRUE)
critval <- 1.96 ## approx 95% CI
upr <- predict_fit$fit + (critval * predict_fit$se.fit)
lwr <- predict_fit$fit - (critval * predict_fit$se.fit)
estimate <- predict_fit$fit
estimate2 <- fit$family$linkinv(estimate)
upr2 <- fit$family$linkinv(upr)
lwr2 <- fit$family$linkinv(lwr)
```

With logistic regression, there's no possibility to create a predictive interval because it would be [0,1] if the probability is between 0.05 and 0.95 and [0,0]/[1,1] otherwise.