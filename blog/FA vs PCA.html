<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Analyse factorielle vs analyse en composantes principales - Justin Savoie</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: Georgia, 'Times New Roman', serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
            max-width: 650px;
            margin: 0 auto;
            padding: 60px 20px;
        }
        
        /* Header with navigation and social icons */
        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 60px;
        }
        
        /* Navigation */
        nav {
            font-size: 16px;
        }
        
        nav a {
            color: #333;
            text-decoration: none;
            margin-right: 30px;
            transition: color 0.3s ease;
        }
        
        nav a:hover {
            color: #0066cc;
        }
        
        nav a.active {
            border-bottom: 1px solid #333;
        }
        
        /* Social Icons */
        .social-icons {
            display: flex;
            gap: 16px;
        }
        
        .social-icons a {
            color: #666;
            font-size: 18px;
            transition: color 0.3s ease;
            text-decoration: none;
        }
        
        .social-icons a:hover {
            color: #0066cc;
        }
        
        /* Main content */
        .content {
            margin-bottom: 60px;
        }
        
        h1 {
            font-size: 24px;
            font-weight: normal;
            margin-bottom: 40px;
        }
        
        p {
            margin-bottom: 20px;
            font-size: 16px;
            line-height: 1.8;
        }
        
        a {
            color: #0066cc;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        /* Blog/Games section - compact style */
        .blog-section {
            display: none;
        }
        .playground-section {
            display: none;
        }
        
        .blog-list {
            margin-top: 40px;
        }
        
        .blog-item {
            margin-bottom: 12px;
            font-size: 16px;
        }
        
        .blog-item .date {
            display: inline-block;
            width: 120px;
            color: #666;
        }
        
        .blog-item a {
            color: #0066cc;
        }
        
        /* Article styles for blog posts */
        article h1 {
            font-size: 28px;
            font-weight: normal;
            margin-bottom: 20px;
            line-height: 1.3;
        }
        
        .post-meta {
            color: #666;
            font-size: 14px;
            margin-bottom: 40px;
        }
        
        h2 {
            font-size: 22px;
            font-weight: normal;
            margin-top: 40px;
            margin-bottom: 20px;
        }
        
        h3 {
            font-size: 18px;
            font-weight: normal;
            margin-top: 30px;
            margin-bottom: 15px;
        }
        
        blockquote {
            border-left: 3px solid #ddd;
            padding-left: 20px;
            margin: 20px 0;
            color: #666;
            font-style: italic;
        }
        
        code {
            background-color: #f5f5f5;
            padding: 2px 5px;
            font-family: 'Courier New', Courier, monospace;
            font-size: 14px;
        }
        
        pre {
            background-color: #f5f5f5;
            padding: 15px;
            margin: 20px 0;
            overflow-x: auto;
            font-size: 14px;
            line-height: 1.4;
        }
        
        pre code {
            background-color: transparent;
            padding: 0;
        }
        
        ul, ol {
            margin-bottom: 20px;
            padding-left: 30px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 20px 0;
        }
        
        /* Footer */
        footer {
            margin-top: 80px;
            padding-top: 40px;
            border-top: 1px solid #eee;
            font-size: 14px;
            color: #666;
        }
        
        /* Responsive */
        @media (max-width: 600px) {
            body {
                padding: 40px 20px;
            }
            
            header {
                flex-direction: column;
                gap: 20px;
                align-items: flex-start;
            }
            
            h1 {
                font-size: 24px;
            }
            
            nav {
                margin-bottom: 0;
            }
            
            nav a {
                margin-right: 20px;
            }
            
            .social-icons {
                gap: 20px;
            }
            
            .blog-item .date {
                display: block;
                width: auto;
                margin-bottom: 5px;
            }
        }
    </style>
    <!-- MathJax config for rendering LaTeX (inline: $...$, display: $$...$$) -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$','$$'], ['\\[','\\]']],
                processEscapes: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<header>
        <nav>
            <a href="/" class="nav-about">About</a>
            <a href="/#blog" class="nav-blog">Blog</a>
            <a href="/#playground" class="nav-playground">Playground</a>
        </nav>
        
        <div class="social-icons">
            <a href="https://github.com/justinsavoie" title="GitHub" target="_blank" rel="noopener">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
                </svg>
            </a>
            <a href="https://twitter.com/justinsavoie" title="Twitter/X" target="_blank" rel="noopener">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/>
                </svg>
            </a>
            <a href="https://www.linkedin.com/in/justin-savoie-a7793932/" title="LinkedIn" target="_blank" rel="noopener">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"/>
                </svg>
            </a>
            <a href="https://scholar.google.ca/citations?user=U4l1gLYAAAAJ&hl=en" title="Google Scholar" target="_blank" rel="noopener">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z"/>
                </svg>
            </a>
            <a href="mailto:savoiejustin99@gmail.com" title="Email">
                <svg width="18" height="18" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M24 5.457v13.909c0 .904-.732 1.636-1.636 1.636h-3.819V11.73L12 16.64l-6.545-4.91v9.273H1.636C.732 21.002 0 20.27 0 19.366V5.457c0-.904.732-1.636 1.636-1.636h1.82l8.544 6.408 8.544-6.408h1.82c.904 0 1.636.732 1.636 1.636z"/>
                </svg>
            </a>
            <a href="/savoiejustin.pdf" title="CV" target="_blank" rel="noopener" style="display: flex; align-items: center;">
                <span style="font-family: Georgia, serif; font-size: 14px; font-weight: normal;">CV</span>
            </a>
        </div>
    </header>
    
    <article>
    <h1>Analyse factorielle vs analyse en composantes principales</h1>
    <div class="post-meta">11 Feb, 2026</div>
    
    <p>Pour réduire la dimensionnalité, on a souvent à choisir entre l'analyse en composantes principales (PCA) et l'analyse factorielle (FA). Souvent, on ne sait pas laquelle choisir. Les deux reposent sur des postulats différents. En science sociale, d'habitude on utilise l'analyse factorielle.</p>
<h4>Analyse en composantes principales / Principal Component Analysis (PCA)</h4>
<ul>
<li>Le seul objectif : réduction de dimensionalité</li>
<li>Modèle : décomposition algébrique de la variance totale</li>
<li>Aucune hypothèse sur une structure latente; sur un concept latent</li>
<li>Les composantes sont des combinaisons linéaires qui maximisent la variance expliquée. C'est tout.</li>
</ul>
<p>Formellement :</p>
<div class="arithmatex">\[
X = Z W'
\]</div>
<p>où <span class="arithmatex">\(Z\)</span> sont les composantes principales. La PCA explique toute la variance, y compris l’erreur spécifique. <sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup></p>
<h4>Analyse factorielle / Factor analysis (FA)</h4>
<p>Parfois aussi appelée analyse factorielle exploratoire / exploratory factor analysis (EFA), pour la distinguer de l'analyse factorielle confirmatoire (CFA). Analyse factorielle exploratoire: on laisse les données révéler librement la structure des facteurs. Analyse factorielle confirmatoire: on teste un modèle factoriel précis défini à l’avance.</p>
<ul>
<li>Objectif de FA : modéliser une structure latente</li>
<li>Modèle probabiliste</li>
<li>Ne s’intéresse qu’à la variance commune (shared variance)</li>
</ul>
<p>Modèle :</p>
<div class="arithmatex">\[
x_i = \lambda_i F + \epsilon_i
\]</div>
<p>où :</p>
<ul>
<li><span class="arithmatex">\(F\)</span> = facteur latent</li>
<li><span class="arithmatex">\(\lambda_i\)</span> = loading</li>
<li><span class="arithmatex">\(\epsilon_i\)</span> = variance spécifique (erreur)</li>
</ul>
<p>La matrice de covariance est :</p>
<div class="arithmatex">\[
\Sigma = \Lambda \Phi \Lambda' + \Psi
\]</div>
<p>En FA, on suppose qu’il existe une variable latente (ex : autoritarisme, nationalisme, etc.) qui génère les réponses observées.</p>
<h4>Comment calculer une FA ?</h4>
<ol>
<li>On estime la matrice de corrélations.</li>
<li>On extrait les facteurs (ML ou minres).</li>
<li>On estime les loadings.</li>
<li>On peut calculer des scores factoriels (les positions des gens sur notre nouvelle échelle de mesure de notre variable latente).</li>
</ol>
<p>Exemple:</p>
<pre><code class="language-r">n &lt;- 500
F &lt;- rnorm(n)

x1 &lt;- 0.8*F + rnorm(n, sd=.6)
x2 &lt;- 0.7*F + rnorm(n, sd=.7)
x3 &lt;- 0.9*F + rnorm(n, sd=.5)
x4 &lt;- rnorm(n)  # bruit pur

dat &lt;- data.frame(x1,x2,x3,x4)

# analyse factorielle 1 facteur
fa1 &lt;- factanal(dat, 1, scores = &quot;regression&quot;)
</code></pre>
<p>Les loadings représentent la contribution d’un facteur latent à une variable observée; dans les modèles orthogonaux standardisés, ils correspondent aux corrélations variable–facteur. Encore une fois, ici c'est compliqué mais l'idée c'est que les loadings représentent à quel point les items sont liés au facteur latent.</p>
<p>Scores factoriels:</p>
<pre><code class="language-r">c(fa1$scores)
</code></pre>
<p>Les scores factoriels sont des estimations de la position des individus sur le facteur latent. Ils dépendent des loadings, de la matrice de corrélation (ou covariance) entre les items, ainsi que de la méthode d’estimation utilisée (régression, Bartlett, Anderson-Rubin, etc.). Intuitivement, on peut les voir comme une combinaison linéaire pondérée des items, où les poids sont dérivés des loadings et de la structure de corrélation.</p>
<p>On utilise souvent FA pour vérifier que les items mesurent bien une même dimension, puis on calcule simplement une moyenne additive parce qu’elle est plus simple à interpréter, plus stable entre échantillons et moins dépendante du modèle. Lorsque les loadings sont similaires, pondérer par des scores factoriels n’apporte presque rien par rapport à une simple moyenne.</p>
<p>En pratique :</p>
<pre><code class="language-r">dat$scale_mean &lt;- rowMeans(dat[,c(&quot;x1&quot;,&quot;x2&quot;,&quot;x3&quot;)])
</code></pre>
<p>Si la structure est unidimensionnelle et cohérente, la moyenne ≈ score factoriel.</p>
<p>Comment choisir le nombre de dimensions?</p>
<pre><code class="language-r">library(psych)
fa.parallel(dat)
</code></pre>
<p>On peut quand même rester à une seule dimension même si fa.parallel propose plus. Ça dépend de notre théorie.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Dire que la PCA “explique toute la variance, y compris l’erreur spécifique” signifie intuitivement qu’elle ne fait aucune distinction entre ce qui, dans une variable, provient d’un construit latent partagé et ce qui relève simplement de son bruit propre ou de ses particularités idiosyncratiques. Imaginons trois items censés mesurer l’autoritarisme : chacun contient une part commune (le trait latent) et une part spécifique (erreur de mesure, ambiguïté de formulation, humeur du répondant, variance propre au contenu précis de l’item). L’analyse factorielle suppose explicitement que seule la variance commune doit servir à estimer le facteur latent, et elle modélise séparément la variance spécifique. La PCA, en revanche, cherche uniquement une combinaison linéaire des variables qui maximise la variance totale observée dans les données, sans se demander d’où vient cette variance ; elle traite donc la variance partagée et la variance spécifique exactement de la même manière. Si un item contient beaucoup de variance idiosyncratique (par exemple parce qu’il est mal formulé ou capte un aspect marginal), cette variance compte quand même dans le calcul de la composante principale. Autrement dit, la PCA compresse l’information statistique disponible, mais elle ne distingue pas structure latente et bruit : elle “explique” toute la dispersion des données, qu’elle soit théoriquement pertinente ou non. Mathématiquement, la distinction vient du fait que la PCA et la FA partent de décompositions différentes de la matrice de covariance (ou de corrélation) des variables observées. En PCA, on prend simplement la matrice de covariance et on fait une décomposition en valeurs propres. Les composantes principales sont des combinaisons linéaires correspondant aux directions de variance maximale, et toute la variance présente dans la matrice de covariance est distribuée entre ces composantes ; il n’y a aucun terme séparé pour l’erreur ou la variance spécifique, car le modèle n’en postule pas l’existence. En analyse factorielle, on impose un modèle où la variance commune est attribuée aux facteurs latents et la variance spécifique est modélisée séparément. Autrement dit, la FA contraint la matrice de covariance à être décomposée en une partie de rang réduit (les facteurs communs) plus un résidu diagonal (le spécifique), alors que la PCA ne sépare rien : elle redistribue toute la variance observée dans des axes orthogonaux sans modèle génératif sous-jacent. La distinction mathématique tient donc au fait que la FA introduit explicitement un terme pour la variance non partagée et estime ses paramètres, tandis que la PCA effectue uniquement une décomposition spectrale de la variance totale, sans hypothèse sur sa provenance. C'est ok de ne pas tout comprendre. C'est réellement compliqué. Il faut juste comprendre l'idée en général.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</article>
    
    <footer>
        <p>© 2025 Justin Savoie</p>
    </footer>
    
    <script>
        // Navigation for index page
        function showSection(section) {
            const aboutSection = document.getElementById('about-section');
            const blogSection = document.getElementById('blog-section');
            const playgroundSection = document.getElementById('playground-section');
            
            if (aboutSection && blogSection && playgroundSection) {
                aboutSection.style.display = 'none';
                blogSection.style.display = 'none';
                playgroundSection.style.display = 'none';
                
                document.querySelectorAll('nav a').forEach(link => {
                    link.classList.remove('active');
                });
                
                if (section === 'about') {
                    aboutSection.style.display = 'block';
                    document.querySelector('.nav-about').classList.add('active');
                } else if (section === 'blog') {
                    blogSection.style.display = 'block';
                    document.querySelector('.nav-blog').classList.add('active');
                } else if (section === 'playground') {
                    playgroundSection.style.display = 'block';
                    document.querySelector('.nav-playground').classList.add('active');
                }
            }
        }
        
        // Handle navigation clicks
        document.querySelector('.nav-about').addEventListener('click', function(e) {
            if (window.location.pathname === '/' || window.location.pathname === '/index.html') {
                e.preventDefault();
                showSection('about');
                history.pushState(null, '', '/');
            }
        });
        
        document.querySelector('.nav-blog').addEventListener('click', function(e) {
            if (window.location.pathname === '/' || window.location.pathname === '/index.html') {
                e.preventDefault();
                showSection('blog');
                history.pushState(null, '', '/#blog');
            }
        });
        
        document.querySelector('.nav-playground').addEventListener('click', function(e) {
            if (window.location.pathname === '/' || window.location.pathname === '/index.html') {
                e.preventDefault();
                showSection('playground');
                history.pushState(null, '', '/#playground');
            }
        });
        
        // Check URL hash on load
        if (window.location.hash === '#blog') {
            showSection('blog');
        } else if (window.location.hash === '#playground') {
            showSection('playground');
        } else if (window.location.pathname === '/' || window.location.pathname === '/index.html') {
            showSection('about');
        }
    </script>
</body>
</html>
