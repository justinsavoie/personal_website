---
title: "Multiple linear regression"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
editor: visual
---


## OLS with matrices

The [linear regression model](https://en.wikipedia.org/wiki/Linear_regression) takes the form: $$y_i=\beta_0+\beta_1x_i+...+\beta_kx_{ik}+\varepsilon = x_i^T\beta+\varepsilon_i, i = 1,...,n$$

In matrix form: $$y=X\beta+\varepsilon$$

This model returns a prediction which is essentially a weighted average of the independent variables, plus an intercept, which represents the expected value of the dependent variable when all predictors are set to zero.

In `R`, let's model `api00` as a function of `enroll`, `meals` and `full`. Let's then break down each part:


```{r}
d <- read.csv("https://stats.idre.ucla.edu/wp-content/uploads/2019/02/elemapi2v2.csv")
fit <- lm(api00 ~  enroll + meals + full, data = d)
X <- as.matrix(cbind(1,d[,c("enroll","meals","full")]))
y <- d$api00
```


The vector of coefficients $\hat{\beta}$  can be obtained using matrix operations: $$(X^TX)^{-1}X^Ty$$


```{r}
betas <- solve(t(X) %*% X) %*% t(X) %*% y
```


The [variance](https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf) of $\hat{\beta}$ is $Var(\hat{\beta})=\hat{\sigma^2}(X^TX)^{-1}$


```{r}
residuals <- y - (X %*% betas)
k <- ncol(X)-1
degrees_of_freedom <- nrow(X) - k - 1
residual_variance <- sum(residuals^2) / degrees_of_freedom 
# residual_variance is the sigma hat squared
betas_cov <- residual_variance * solve(t(X) %*% X)
betas_se <- sqrt(diag(betas_cov))
cbind(c(betas),unname(c(betas_se)))
coef(summary(fit))[,1:2]
```


## Maximum likelihood estimation

With $y=X\beta+\varepsilon$:

$$L(\beta, \sigma^2 | Y, X) = \prod_{i=1}^{n} {\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {y_i-X_i\beta }{\sigma }}\right)^{2}}$$

With a [little bit of math](https://www.statlect.com/fundamentals-of-statistics/normal-distribution-maximum-likelihood) we can get the log likelihood:

$$lnL(\beta, \sigma^2 | Y, X) = -\frac{n}{2}\ln(2\pi)-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(y_i-X_i\beta)$$

This is something we can optimize in R:


```{r}
betas_params_initial <- rnorm(4)
sigma_params_initial <- rlnorm(1)
params <- c(betas_params_initial,sigma_params_initial)
to_optim <- function(pars,y,x1,x2,x3){
  b0 <- pars[1]
  b1 <- pars[2]
  b2 <- pars[3]
  b3 <- pars[4]
  sigma_u <- pars[5]
  sigma <- exp(sigma_u)
  mu_vector <- b0+b1*x1+b2*x2+b3*x3
  n<-length(y)
  -sum(sapply(1:n, function(x) {dnorm(y[x],mu_vector[x],sigma,log=TRUE)}))
  # alternatively without using dnorm
  #log_density <- function(x, mean, sd) {-0.5 * log(2 * pi) - log(sd) - 0.5 * ((x - mean)/sd)^2}
  #-sum(sapply(1:n, function(x) {log_density(y[x], mu_vector[x], sigma)}))
}
optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="L-BFGS-B")$par
coef(summary(fit))[,1]
```


Evidently, optimization does not always work so it makes sense to use already tested functions and packages ...


```{r}
(nelder_mead_params <- optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="Nelder-Mead")$par)
(BFGS_params <- optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="BFGS")$par)
coef(summary(fit))[,1]
```


I'm not sure why Nelder-Mead is so bad. I guess it found a local minimum.

To obtain the standard errors, we need the Hessian matrix of the second derivatives of the log-likelihood function evaluated at the maximum likelihood estimates. Here's where I need to brush up. I need to write something in the future about gradients, jacobians, hessians and Newton's method (see below for dirty code) for MLE.

Anyways, the inverse of this matrix is an estimate of the covariance matrix of the parameter estimates.


```{r}
fit_optim <- optim(params,to_optim,y=d$api00,x1=d$enroll,x2=d$meals,x3=d$full,method="L-BFGS-B",hessian = TRUE)
cov_beta_hat <- solve(fit_optim$hessian)
(se_beta_hat <- sqrt(diag(cov_beta_hat)))
coef(summary(fit))[,2]
```


We can also do the optimization by hand using [Netwon-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization). It involves updating parameters by subtracting the multiplication of the inverse of the Hessian with the gradient vector.


```{r}
betas <- params[1:4]

for (i in 1:5){
  yhat = X %*% betas
  gradients <- t(X) %*% (y-yhat)
  hessian <- - t(X) %*% X
  betas <- betas - solve(hessian) %*% gradients
}

residuals <- y - X %*% betas
sigma2_hat <- sum(residuals^2) / (length(y) - length(betas))
cov_beta_hat <- sigma2_hat * solve(-hessian)
se_beta_hat <- sqrt(diag(cov_beta_hat))
cbind(c(betas),unname(c(betas_se)))
coef(summary(fit))[,1:2]
```


Finally, no one uses that in applied statistics in the social sciences but it's widely used in machine learning so it's helpful to know ; here's how to get the point estimates of the coefficients using gradient descent:

For gradient descent, we need to scale.


```{r}
X <- as.matrix(cbind(1, d[, c("enroll", "meals", "full")]))
X[,-1] <- scale(X[,-1])
y <- scale(d$api00)
m <- length(y)

# Settings
alpha <- 0.001  # Learning rate
num_iterations <- 10000
betas <- matrix(rnorm(4,0,0.1), ncol(X), 1)  # Initialize beta values

# Gradient Descent in a for loop
for(i in 1:num_iterations){
  # Compute the prediction
  prediction <- X %*% betas
  
  # Compute the error
  error <- prediction - matrix(y, ncol=1)
  
  # Update betas
  for(j in 1:ncol(X)){
    gradient <- t(X[, j]) %*% error
    betas[j,] <- betas[j,] - alpha * (1/m) * gradient
  }
}

round(print(betas),3)
round(coef(lm(y~X[,-1])),3)

```


It ultimately worked but I had to play with alpha and the number of iterations. It doesn't make sense to use gradient descent for the kind of applied statistical work we do in social science.

## Additional statistics lm()


```{r}
summary(fit)

(R2 <- 1 - (sum((fit$model$api00 - predict(fit))^2) /
  sum((fit$model$api00-mean(fit$model$api00))^2)))
(R2adj <- 1 - (1 - R2) * ((nrow(fit$model) - 1) / 
                           (nrow(fit$model) - length(fit$coefficients[-1]) - 1)))
(Residuals <- quantile(fit$residuals,c(0,0.25,0.5,0.75,1)))
(tstats <- coef(summary(fit))[,1] / 
  coef(summary(fit))[,2])
(d_free <- nrow(fit$model) - length(fit$coefficients))
(pvalues <- 2 * (1 - pt(abs(tstats), df=d_free)))

(Res_se <- sqrt(sum(fit$residuals^2) /
       (nrow(fit$model)-(1+length(fit$coefficients[-1])))))

SSE=sum(fit$residuals^2)
SSyy=sum((fit$model$api00-mean(fit$model$api00))^2)
k<-length(fit$coefficients)-1
Fstat <- ((SSyy-SSE)/k) / (SSE/(400-(3+1)))

(Fstat_pval <- 1-pf(Fstat,3,396))
```


## Additional statistics glm() for the Gaussian family


```{r}
fit_glm <- glm(api00 ~  enroll + meals + full, data = d, family='gaussian')
summary(fit_glm)

quantile(fit_glm$residuals,c(0,0.25,0.5,0.75,1))

(Dispersion <- sum(fit$residuals^2) /
  (nrow(fit$model)-(1+length(fit$coefficients[-1]))))
```

It's possible to get the loglik from brute force:

```{r}
(loglikMLE <- sum(
  sapply(1:400, function(i){
    log(dnorm(fit_glm$model$api00[i],
              coef(fit_glm)[[1]] + sum(fit_glm$model[i,2:4] * coef(fit_glm)[2:4]),
              sqrt(Dispersion)))
  })
))
logLik(fit_glm)
```

It's possible to get the loglik using a formula derived from summation:

```{r}
(nulldeviance <- sum((d$api00 - mean(d$api00))^2))
(residualdeviance <- sum(residuals(fit_glm)^2))

(loglikDirect <-
    (-400/2) * log(2*pi) - (400/2) * log(Dispersion) -
    (1/(2*Dispersion)) * residualdeviance)
```


4 coefficient parameters + dispersion of Gaussian = 5


```{r}
(AIC = 2*5 - 2*logLik(fit_glm))
BIC(fit_glm)
unname(5*log(400) - 2*logLik(fit_glm))

```


I want to discuss Fisher Scoring in another entry.
