---
title: "Logistic regression"
format:
  html:
    embed-resources: true
    toc: true
    toc-location: left
editor: visual
---

## Definition

Rather than being drawn from a normal distribution (where $\mu=\beta X$) like in linear regression, the outcome variable in [logistic regression is drawn from a Bernouilli]((https://en.wikipedia.org/wiki/Logistic_regression#Definition)).

$$p(x)={\frac {1}{1+e^{-(\beta _{0}+\beta _{1}x)}}}$$ The logit function, or the log of the odds is defined as $$logit\:p(x)=\ln \left({\frac {p(x)}{1-p(x)}}\right)=\beta _{0}+\beta _{1}x$$

The likelihood (the probability of observing the data) is $$L=\prod _{k:y_{k}=1}p_{k}\,\prod _{k:y_{k}=0}(1-p_{k})$$

The log likelihood is

$$\ell =\sum _{k:y_{k}=1}\ln(p_{k})+\sum _{k:y_{k}=0}\ln(1-p_{k})$$

## Estimation

For example, in R,

$$p(vs=1) \sim Bernouilli(logit(\beta0+\beta1*mpg+\beta2*hp))$$

```{r}
fit<-glm(vs~mpg+hp,mtcars,family='binomial')
```

```{r}
pars_init <- rnorm(3)
y <- mtcars$vs
X <- as.matrix(cbind(1, mtcars[, c("mpg", "hp")]))
to_optim <- function(pars, y, X) {
  betas <- pars
  t <- X %*% betas
  p <- plogis(t)
  ll1 <- sum(log(p[y == 1] + 1e-10))
  ll0 <- sum(log(1 - p[y == 0] + 1e-10))
  return(-(ll1 + ll0))
}
coef(fit)
optim(pars_init, to_optim, y = y, X = X,method = "BFGS")$par
```

It doesn't work. Looks like it's an optimization problem.

Scaling fixes it.

```{r,message=FALSE}
library(tidyverse)
X <- as.matrix(cbind(1, mtcars[, c("mpg", "hp")]%>%mutate_all(scale)))
fit_s<-glm(vs~scale(mpg)+scale(hp),mtcars,family='binomial')
fit_optim <- optim(pars_init, to_optim, y = y, X = X,method = "BFGS",hessian = TRUE)
cov_beta_hat <- solve(fit_optim$hessian)
se_beta_hat <- sqrt(diag(cov_beta_hat))
coef(summary(fit_s))[,1:2]
cbind(fit_optim$par,se_beta_hat)

```

Doing the optimization by hand with Newton Raphson

```{r}
newton_raphson_se <- function(pars, y, X, max_iter=100, tol=1e-06) {
  betas <- pars
  for(i in 1:max_iter) {
    p <- plogis(X %*% betas)
    W <- diag(as.vector(p * (1 - p)))
    gradient <- t(X) %*% (y - p)
    hessian <- -t(X) %*% W %*% X
    betas_new <- betas - solve(hessian, gradient)
    if(sum(abs(betas_new - betas)) < tol) {
      hessian_at_mle <- -t(X) %*% diag(as.vector(p * (1 - p))) %*% X
      cov_beta <- solve(-hessian_at_mle)
      se_beta <- sqrt(diag(cov_beta))
      return(list(coefficients=betas_new, std_error=se_beta, covariance=cov_beta))
    }
    betas <- betas_new
  }
  stop("Algorithm did not converge")
}
pars_init <- rnorm(3,0,0.1)
nr <- newton_raphson_se(pars_init, y, X)
coef(summary(fit_s))[,1:2]
cbind(nr$coefficients,nr$std_error)

```
